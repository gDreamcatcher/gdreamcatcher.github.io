---
layout:     post
title:      "特征提取服务"
subtitle:   "高效利用GPU，快速提取图片特征"
author:     "gdream"
header-img: "img/post-bg-2021.jpg"
catalog: true
tags:
    - else
---



# 特征提取服务

> 本文涉及知识点：
1、多进程编程以及进程间数据交互
2、数据预处理
3、提高GPU使用率

## 多进程编程
什么是进程? 进程是程序的实体, 是线程的容器, 是**计算机进行资源分配和调度的基本单位**. 程序是一系列指令、数据及其组织形式的描述。**线程是独立调度和分派的基本单位**，一个进程中可以包含多个线程并行执行不同的任务。

python多线程编程主要使用 `multiprocessing` 库下的 ` Process, Queue`, `Process`允许我们创建一个子进程，`Queue`保证多进程间数据是安全的
```python
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```

## 提高GPU利用率
这个真的没想到，只需要增加`batch_size`就可以了，另外注意不要把耗时的操作放到模型提特征的循环中

## 完整代码如下
> 现在还只是离线提数据，后续希望可以修改成在线实时的, 感觉有点困难(^_^)


```python
import os
import faiss
import struct
import ctypes
from datetime import datetime
from skimage import io
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Process, Queue
import tensorflow as tf
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input

_BATCH_SIZE = 256

image_queue = Queue(1000)
feature_queue = Queue(1000)


def read_image(image_path, q):
    def foo(line):
        line = line.strip()
        image_url = line.split(',')[-1]
        image = io.imread(image_url)
        if len(image.shape) != 3 or image.shape[2] != 3 or image.shape[0] != 800 or image.shape[1] != 800:
            return
        q.put((line.strip(), image))

    with open(image_path, 'r') as f:
        with ThreadPoolExecutor(100) as executor:
            executor.map(foo, f)
    q.put(None)


def dataset():
    def generate():
        while True:
            res = image_queue.get()
            if res is None:
                return
            yield res

    def deal_image(img):
        """Load an image from a path and resize it."""
        image_size, num_channels, interpolation = ((224, 224), 3, 'bilinear')
        img = tf.image.resize(img, image_size, method=interpolation)
        img.set_shape((image_size[0], image_size[1], num_channels))
        return img

    def decode_label(url):
        return url

    path_ds = tf.data.Dataset.from_generator(generator=generate, args=[], output_types=(
        tf.string, tf.uint8), output_shapes=((), (800, 800, 3)))
    img_ds = path_ds.map(lambda x, y: (decode_label(x), deal_image(
        y)), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)
    ds = img_ds.prefetch(tf.data.AUTOTUNE).batch(_BATCH_SIZE)
    return ds


def extract(out_queue, gpu):
    os.environ['CUDA_VISIBLE_DEVICES'] = gpu
    gpus = tf.config.experimental.list_physical_devices('GPU')
    tf.config.experimental.set_memory_growth(gpus[0], True)
    # tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])
    model = VGG16(weights='imagenet', pooling='max', include_top=False)
    ds = dataset()
    count = 0
    for urls, x in ds:
        count = count + _BATCH_SIZE
        if count / _BATCH_SIZE % 10 == 0:
            print('{} extract finishing: {}, image_queue: {}, feature_queue: {}'.format(
                datetime.now(), count, image_queue.qsize(), feature_queue.qsize()))
        x = preprocess_input(x)
        features = model.predict(x)
        out_queue.put((urls, features))
    out_queue.put(None)


def save_feature(input_queue, url_path, feature_path):
    url_file = open(url_path, 'w')
    feature_file = open(feature_path, 'wb')
    s = struct.Struct('f')
    prebuffer = ctypes.create_string_buffer(512*s.size)
    while True:
        res = input_queue.get()
        if res is None:
            break
        urls, features = res
        url_list = urls.numpy().tolist()
        features = features/np.linalg.norm(features, axis=1)[:, None]
        for line, f in zip(url_list, features):
            url_file.write(line.decode('utf-8')+'\n')
            struct.pack_into('512f', prebuffer, 0, *f)
            feature_file.write(prebuffer)
    url_file.close()
    feature_file.close()


def search(data_url_path, data_feature_path, query_url_path, query_feature_path, save_path):
    with open(data_url_path, 'r') as f:
        data_url_list = f.readlines()
    with open(query_url_path, 'r') as f:
        query_url_list = f.readlines()
    with open(data_feature_path, 'rb') as f:
        data_features = np.fromfile(f, dtype=np.float32)
    with open(query_feature_path, 'rb') as f:
        query_features = np.fromfile(f, dtype=np.float32)
    f = open(save_path, 'w')
    d = 512
    index = faiss.IndexFlatL2(d)   # build the index
    index.add(data_features.reshape((-1, 512)))
    D, I = index.search(query_features.reshape((-1, 512)), 10)
    count = 0
    for i, row in enumerate(D):
        count = count + 1
        if count % 10000 == 0:
            print('{} index finishing: {}'.format(datetime.now(), count))
        for j, distance in enumerate(row):
            if distance <= 0:
                f.write("{},{}\n".format(
                    query_url_list[i].strip(), data_url_list[I[i][j]].strip()))
    f.close()


def parse_args():
    import argparse
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--GPU', type=str, default="0", help='data path')
    parser.add_argument('--method', type=str,
                        default="extract", help='data path')
    parser.add_argument('--data_path', type=str, default="", help='data path')
    parser.add_argument('--url_path', type=str, default="", help='data path')
    parser.add_argument('--feature_path', type=str,
                        default="", help='data path')
    parser.add_argument('--query_url_path', type=str,
                        default="", help='data path')
    parser.add_argument('--query_feature_path', type=str,
                        default="", help='data path')
    parser.add_argument('--query_path', type=str,
                        default="", help='query path')
    parser.add_argument('--save_path', type=str, default="", help='save path')
    args = parser.parse_args()
    return args


if __name__ == '__main__':
    args = parse_args()
    if args.method == 'extract':
        p = Process(target=read_image, args=(args.data_path, image_queue, ))
        p.start()
        p = Process(target=extract, args=(feature_queue, args.GPU))
        p.start()
        save_feature(feature_queue, args.url_path, args.feature_path)
    elif args.method == 'search':
        search(args.url_path, args.feature_path, args.query_url_path,
               args.query_feature_path, args.save_path)
    else:
        print("{} not found".format(args.method))

```
