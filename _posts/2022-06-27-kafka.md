---
layout:     post
title:      "kafka 阅读记录"
author:     "gdream"
header-img: "img/post-bg-2021.jpg"
catalog: true
tags:
    - java
---

# kafka阅读记录

## producer
简单示例
```java
 Properties props = new Properties();
 props.put("bootstrap.servers", "localhost:9092");
 props.put("acks", "all");
 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
 props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

 Producer<String, String> producer = new KafkaProducer<>(props);
 for (int i = 0; i < 100; i++)
     producer.send(new ProducerRecord<String, String>("my-topic", Integer.toString(i), Integer.toString(i)));

 producer.close();
```
- kafka的生产者客户端是线程安全的，多个线程使用同一个客户端的性能要优于多个线程使用不同的客户端;
- 生产者客户端提交`records`是异步过程，异常关闭客户端会导致消息丢失(停止服务前需要先手动关闭`producer client`, 并等待关闭完成，不然在缓存中还没有发送到server的记录将会丢失);
- 生产者发送消息失败会根据设置的 [retries](https://kafka.apache.org/documentation.html#producerconfigs_retries) 值来进行重试的，该值默认是2147483647。注意：如果生产请求配置了 `delivery.timeout.ms` 那么当超时后将不再进行重试;
- `linger.ms` 该参数为0时，即使发送队列中的记录没有到`batch.size`, 消息也会立马被发送到服务端。所以如果想减少发送请求的数量，可以将该参数`linger.ms`设置的大于0，代表每次发送前会等待n毫秒接收新的`record`。
- `buffer.memory` 该参数设置发送队列的大小，当超过发送队列的限制后，send请求会发生阻塞直到`max.block.ms` 超时。
- kafka 0.11版本后支持两种模式：幂等模式和事务模式
  - 幂等模式下同一个record不会被发送两次，因此需要应用层杜绝消息重发的事情发生；
  - 事务模式重试次数自动变成int的最大值，使用方式如下：
  ```java
    Properties props = new Properties();
    props.put("bootstrap.servers", "localhost:9092");
    props.put("transactional.id", "my-transactional-id");
    Producer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());

    producer.initTransactions();

    try {
        producer.beginTransaction();
        for (int i = 0; i < 100; i++)
            producer.send(new ProducerRecord<>("my-topic", Integer.toString(i), Integer.toString(i)));
        producer.commitTransaction();
    } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {
        // We can't recover from these exceptions, so our only option is to close the producer and exit.
        producer.close();
    } catch (KafkaException e) {
        // For all other exceptions, just abort the transaction and try again.
        producer.abortTransaction();
    }
    producer.close();
  ```
- send() 是异步执行的，如果想同步的可以在send后加一个get，如：
  ```java
    byte[] key = "key".getBytes();
    byte[] value = "value".getBytes();
    ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>("my-topic", key, value)
    producer.send(record).get();
  ```
- send后执行一个callback
  ```java
    ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>("the-topic", key, value);
    producer.send(myRecord,
                new Callback() {
                    public void onCompletion(RecordMetadata metadata, Exception e) {
                        if(e != null) {
                            e.printStackTrace();
                        } else {
                            System.out.println("The offset of the record we just sent is: " + metadata.offset());
                        }
                    }
                });
  ```

  